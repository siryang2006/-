{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siryang2006/-/blob/master/LLM%E5%AD%A6%E4%B9%A0_%E4%BB%8E0%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%87%AA%E5%B7%B1%E7%9A%84LLM_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 动手学习-从0构建自己的LLM"
      ],
      "metadata": {
        "id": "cmxbvB4YEP8o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np2VfRceEKmo"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import pandas as pd\n",
        "import urllib.request"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # 创建一个字典用于存储config\n",
        "MASTER_CONFIG = {\n",
        "    # 参数放这里\n",
        "}"
      ],
      "metadata": {
        "id": "zXHnN35jEW_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 下载一个吴承恩版本的西游记原文数据集\n",
        "url = \"https://raw.githubusercontent.com/mc112611/PI-ka-pi/main/xiyouji.txt\"\n",
        "file_name = \"xiyouji.txt\"\n",
        "urllib.request.urlretrieve(url, file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdrJRfFPEYX4",
        "outputId": "22df866c-cc55-4b7e-b67f-76fd85cb994d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('xiyouji.txt', <http.client.HTTPMessage at 0x796071a0a590>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 读数据\n",
        "lines = open(\"xiyouji.txt\", 'r').read()\n",
        "\n",
        "# 创建简易版词表（字符级）\n",
        "vocab = sorted(list(set(lines)))\n",
        "\n",
        "# 查看词表前n个字符\n",
        "head_num=50\n",
        "print('词表前{}个:'.format(head_num), vocab[:head_num])\n",
        "\n",
        "print('词表大小:', len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piqG4JC6EZnG",
        "outputId": "9b640a35-95fd-4b63-a532-ea262852bad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "词表前50个: ['\\n', ' ', '!', '\"', '#', '*', ',', '.', '—', '‘', '’', '“', '”', '□', '、', '。', '《', '》', '一', '丁', '七', '万', '丈', '三', '上', '下', '不', '与', '丑', '专', '且', '丕', '世', '丘', '丙', '业', '丛', '东', '丝', '丞', '丢', '两', '严', '丧', '个', '丫', '中', '丰', '串', '临']\n",
            "词表大小: 4325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 将词表编码成为数字，普通的整数\n",
        "itos = {i: ch for i, ch in enumerate(vocab)}\n",
        "\n",
        "# 双向映射\n",
        "stoi = {ch: i for i, ch in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "famPLEarEfro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 编码器（青春版）\n",
        "def encode(s):\n",
        "    return [stoi[ch] for ch in s]\n",
        "\n",
        "# 解码器（青春版）\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l])\n",
        "\n",
        "# 来试一下这个高端的编解码器\n",
        "decode(encode(\"悟空\"))\n",
        "encode(\"悟空\")\n",
        "# cing~ 效果拔群！"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxnpjDWVOvSZ",
        "outputId": "ae7208e5-8c6b-41b0-c0c1-51650a898a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1318, 2691]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 对全文进行编码，并映射成为tensor\n",
        "dataset = torch.tensor(encode(lines), dtype=torch.int16)\n",
        "\n",
        "# 看一下形状，实际上就是多少个字符，一共65万个字符\n",
        "print(dataset.shape)\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQefIROyOwu4",
        "outputId": "c1138da5-1082-4765-b1f2-fc5171e589c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([658298])\n",
            "tensor([   0, 4319, 1694,  ...,   12,    0,    0], dtype=torch.int16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 构建batch\n",
        "def get_batches(data, split, batch_size, context_window, config=MASTER_CONFIG):\n",
        "    # 切分训练集，验证集，测试集，比例为，训练80%，验证10%，测试10%\n",
        "    train = data[:int(0.8 * len(data))]\n",
        "    val = data[int(0.8 * len(data)): int(0.9 * len(data))]\n",
        "    test = data[int(0.9 * len(data)):]\n",
        "\n",
        "    # 将全部的训练数据作为batch，验证集，测试集也换个变量存储（单纯为了方便看）\n",
        "    batch_data = train\n",
        "    if split == 'val':\n",
        "        batch_data = val\n",
        "    if split == 'test':\n",
        "        batch_data = test\n",
        "\n",
        "    # 这里需要学习torch.randint，生成大小为batch_size，内部数值为随机整数的tensor。生成随机数数值域为[0,训练集字符数量-滑动窗口大小-1]之间的整数\n",
        "    # 详情可以参考官方文档，或者这个博客：https://blog.csdn.net/qq_41813454/article/details/136326473\n",
        "    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))\n",
        "    # print('ix输出:')\n",
        "    # print(ix)\n",
        "\n",
        "\n",
        "    # 这里需要学习torch.stack，执行操作类似于python的zip关键字，只不过操作对象是tensor张量，指定任意维度的张量进行组合\n",
        "    # 详情参考官方文档，或者这个博客：https://blog.csdn.net/dongjinkun/article/details/132590205\n",
        "\n",
        "    # 这里x作为特征，y作为预测值，因为文本生成任务是根据前n个字符，去推理后面的1个字符，因此y的构造会使窗口在保持原大小的基础上向后移一位\n",
        "    # 通过滑动窗口，对batch_data中的训练数据，进行随机取样，相当于随机选择训练数据。\n",
        "    # 在原65万多个字符中，随机选取一个字符作为开始，并以这个开始点，向后选取滑动窗口个数的字符，作为训练数据，向后移一位就是其目标值。  因此ix的构造不能超出index。\n",
        "    x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()\n",
        "    y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()\n",
        "\n",
        "    # 返回特征值，目标值\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "CVIIMsruPCQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 根据上面构造的get_batchs()函数，更新参数字典。\n",
        "MASTER_CONFIG.update({\n",
        "    'batch_size': 8,          # 不解释\n",
        "    'context_window': 16,      # 滑动窗口采样，设置采样大小\n",
        "    'vocab_size':4325         # 咱们的西游记数据集，一共包含4325个不重复的汉字，标点符号\n",
        "})"
      ],
      "metadata": {
        "id": "9n7esIVbP45W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取训练数据\n",
        "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
        "\n",
        "# 因为是随机生成的采样，我们可以看一下数据，其中每个采样数据，来自于原文随机的起始点，每个元组为一个（x,y），可以观察每个x和y的首位去直观感受一下滑动窗口执行的操作\n",
        "decoded_samples = [(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))]\n",
        "\n",
        "print(decoded_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWqRhpn2P6tu",
        "outputId": "67056b6f-d01e-4b8a-9e59-8683b8bbec30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('说，吓得跪倒在地，道：“郎君啊，', '，吓得跪倒在地，道：“郎君啊，你'), ('把师\\n父抱下马来，回头便走。那条', '师\\n父抱下马来，回头便走。那条龙'), ('家。又问那里安歇，何也？”猪八戒', '。又问那里安歇，何也？”猪八戒道'), ('邪败阵逃生，回归本宅，众妖接到宫', '败阵逃生，回归本宅，众妖接到宫中'), ('久，就于雪洞里坐下，对邻叟道取经', '，就于雪洞里坐下，对邻叟道取经之'), ('千之喜！”那国王正与三藏膳毕清谈', '之喜！”那国王正与三藏膳毕清谈，'), ('，往前正走，被行者撞个满怀，掣出', '往前正走，被行者撞个满怀，掣出\\n'), ('不曾丢瓦抛砖伤佛殿，\\n阿罗脸上剥', '曾丢瓦抛砖伤佛殿，\\n阿罗脸上剥真')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 构造一个评估函数\n",
        "@torch.no_grad()\n",
        "def evaluate_loss(model, config=MASTER_CONFIG):\n",
        "    # 评估结果存储变量\n",
        "    out = {}\n",
        "\n",
        "    # 将模型置为评估模式\n",
        "    model.eval()\n",
        "\n",
        "    # 分别会在训练集和验证集里通过get_batchs()函数取评估数据\n",
        "    for split in [\"train\", \"val\"]:\n",
        "\n",
        "        losses = []\n",
        "\n",
        "        # 评估10个batch\n",
        "        for _ in range(10):\n",
        "            # 拿到特征值（输入数据），以及目标值（输出数据）\n",
        "            xb, yb = get_batches(dataset, split, config['batch_size'], config['context_window'])\n",
        "\n",
        "            # 把拿到的数据丢进模型，得到loss值\n",
        "            _, loss = model(xb, yb)\n",
        "\n",
        "            # 更新loss存储\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        # 这里就是大家经常在控制台看到的 \"train_loss\"  \"valid_loss\"由来\n",
        "        out[split] = np.mean(losses)\n",
        "\n",
        "    # 评估完了，别忘了把模型再置回训练状态，下一个epoch还要继续训练呢\n",
        "    model.train()\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "BiamzI4tP71e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 在进行分析LlaMa架构分析之前，我们从最简单的文本生成模型开始创建，然后在最简单的文本生成模型的基础上，把LlaMa的RSM，Rope等一点点添加进去。为此我们先：\n",
        "# 创建一个有毛病的模型架构\n",
        "# 分析一下这个架构（其实也没什么分析的）\n",
        "class StupidModel(nn.Module):\n",
        "    def __init__(self, config=MASTER_CONFIG):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # embedding层，输入：词表大小，输出：维度大小\n",
        "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "\n",
        "        # 创建线性层用于捕捉特征关系\n",
        "        # 下面突击检查：这玩意是不是隐藏层！线性层堆叠越多是不是越好！堆叠越多是不是更计算开销越大！\n",
        "        # LlaMa使用的激活函数是SwiGLU，目前在这个斯丢匹德模型架构里面先用Relu\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(config['d_model'], config['d_model']),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config['d_model'], config['vocab_size']),\n",
        "        )\n",
        "\n",
        "        # 这个命令可以背一下，或者复制粘贴到自己的学习笔记。 因为这行命令会直接帮你查看模型的参数量。\n",
        "        # 否则要么自己手算，要么就是听别人讲某某模型 7B  20B  108B   有了这个命令，你就能直接查看你创建的模型参数量多少\n",
        "        print(\"模型参数量：\", sum([m.numel() for m in self.parameters()]))"
      ],
      "metadata": {
        "id": "M3gWpmDcQEfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 为我们创建的小模型添加前向传播\n",
        "class SimpleBrokenModel(nn.Module):\n",
        "    # init里的跟上面一样，没变化\n",
        "    def __init__(self, config=MASTER_CONFIG):\n",
        "      super().__init__()\n",
        "      self.config = config\n",
        "      self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "      self.linear = nn.Sequential(\n",
        "          nn.Linear(config['d_model'], config['d_model']),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(config['d_model'], config['vocab_size']),\n",
        "      )\n",
        "\n",
        "\n",
        "\n",
        "      # 添加前向传播函数\n",
        "    def forward(self, idx, targets=None):\n",
        "        # 实例化embedding层，输入映射为id的数据，输出嵌入后的数据\n",
        "        x = self.embedding(idx)\n",
        "\n",
        "        # 线性层承接embedding层输出的数据\n",
        "        a = self.linear(x)\n",
        "\n",
        "        # 对线性层输出的数据在最后一个维度，做softmax，得到概率分布\n",
        "        logits = F.softmax(a, dim=-1)\n",
        "\n",
        "        # 如果有目标值（也就是我们前面的y），则计算通过交叉熵损失计算loss结果。给输出的概率矩阵变个形状，再给目标值变个形状。  统一一下输入输出，然后计算loss。其中最后一维代表着一条数据。\n",
        "        # 此处需要了解tensor.view()函数，带上几何空间想象力去想一下矩阵的形状。\n",
        "        if targets is not None:\n",
        "\n",
        "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
        "            return logits, loss\n",
        "\n",
        "        # 如果没有目标值，则只返回概率分布的结果\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "        # 查看参数量\n",
        "        print(\"模型参数量：\", sum([m.numel() for m in self.parameters()]))"
      ],
      "metadata": {
        "id": "vwPGKdTkQF7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 这里我们设置这个模型为128维的embedding\n",
        "MASTER_CONFIG.update({\n",
        "    'd_model': 128,\n",
        "})\n",
        "\n",
        "# 实例化模型，传参\n",
        "model = SimpleBrokenModel(MASTER_CONFIG)\n",
        "\n",
        "# 再看看参数量\n",
        "print(\"咱们的模型这么多参数量:\", sum([m.numel() for m in model.parameters()]))\n",
        "# 于是乎，我们创建了一个1128307个参数的模型，上面参数想怎么改，自己改！电脑不会爆炸！"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_SOupaeQG82",
        "outputId": "932b32f9-dbcc-40c8-ce5e-fc068dad1592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "咱们的模型这么多参数量: 1128037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取训练的特征数据与目标数据\n",
        "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
        "\n",
        "# 扔进模型获取概率分布矩阵与loss\n",
        "logits, loss = model(xs, ys)\n",
        "loss"
      ],
      "metadata": {
        "id": "jqURXOvWRMZX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6566f1c6-d8d7-4ecc-9060-66b91fae6329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(8.3722, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 更新参数，训练伦次，batch_size，log日志打印步长\n",
        "MASTER_CONFIG.update({\n",
        "    'epochs': 1000,\n",
        "    'log_interval': 10,      # 每10个batch打印一次log\n",
        "    'batch_size': 32,\n",
        "})\n",
        "\n",
        "# 实例化模型\n",
        "model = SimpleBrokenModel(MASTER_CONFIG)\n",
        "\n",
        "# 创建一个Adam优化器，基础知识，\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),      # 优化器执行优化全部的模型参数\n",
        ")"
      ],
      "metadata": {
        "id": "fv6E7A0yRecM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 构建训练函数\n",
        "def train(model, optimizer, scheduler=None, config=MASTER_CONFIG, print_logs=False):\n",
        "    # loss存储\n",
        "    losses = []\n",
        "\n",
        "    # 训练时间记录开始时间\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 循环训练指定epoch的轮数\n",
        "    for epoch in range(config['epochs']):\n",
        "        # 优化器要初始化啊，否则每次训练都是基于上一次训练结果进行优化，效果甚微\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 获取训练数据\n",
        "        xs, ys = get_batches(dataset, 'train', config['batch_size'], config['context_window'])\n",
        "\n",
        "        # 前向传播计算概率矩阵与loss\n",
        "        logits, loss = model(xs, targets=ys)\n",
        "\n",
        "        # 反向传播更新权重参数，更新学习率优化器\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # 如果提供学习率调度器，那么学习率会通过调度器进行修改，比如学习率周期性变化，或者梯度减小，增加，具体策略需要综合考虑进行设置，详情自行查询，关键字：lr_scheduler\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        # 打印log\n",
        "        if epoch % config['log_interval'] == 0:\n",
        "            # 训练时间\n",
        "            batch_time = time.time() - start_time\n",
        "\n",
        "            # 执行评估函数，在训练集和验证集上计算loss\n",
        "            x = evaluate_loss(model)\n",
        "\n",
        "            # Store the validation loss\n",
        "            losses += [x]\n",
        "\n",
        "            # 打印进度日志\n",
        "            if print_logs:\n",
        "                print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config['epochs'] - epoch)/config['log_interval'] :.3f}\")\n",
        "\n",
        "            # 重置开始时间，用于计算下一轮的训练时间\n",
        "            start_time = time.time()\n",
        "\n",
        "            # 打印下一轮的学习率，如果使用了lr_scheduler\n",
        "            if scheduler:\n",
        "                print(\"lr: \", scheduler.get_lr())\n",
        "\n",
        "    # 上面所有epoch训练结束，打印最终的结果\n",
        "    print(\"Validation loss: \", losses[-1]['val'])\n",
        "\n",
        "    # 返还每一步loss值的列表，因为我们要画图，返还的是loss迭代的图像\n",
        "    return pd.DataFrame(losses).plot()\n",
        "\n",
        "# 启动训练\n",
        "train(model, optimizer)"
      ],
      "metadata": {
        "id": "BI4_4YqKSdEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上面那个训练框架存在一些问题。  回到前向传播的代码，也就是forward()中。 我们使用了 logits = F.softmax(a, dim=-1)   对线性层输出的结果做了一次概率分布的计算。  而loss的计算选择了交叉熵损失， 目标值的词表映射结果是整数，而模型输出的logits是概率矩阵。  为了使loss计算更精确，我们需要将softmax去除。  以保证交叉熵损失的计算效果更好。"
      ],
      "metadata": {
        "id": "Qml9Z-ZHrDpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 拿掉softmax，logits改为获取最后一个线性层输出的结果，不进行softmax计算概率分布。\n",
        "# 因此将这个架构取名为：不那么蠢的模型架构\n",
        "class SimpleNotStupidModel(nn.Module):\n",
        "    def __init__(self, config=MASTER_CONFIG):\n",
        "      super().__init__()\n",
        "      self.config = config\n",
        "      self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "      self.linear = nn.Sequential(\n",
        "          nn.Linear(config['d_model'], config['d_model']),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(config['d_model'], config['vocab_size']),\n",
        "      )\n",
        "      print(\"Model parameters:\", sum([m.numel() for m in self.parameters()]))\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        x = self.embedding(idx)\n",
        "\n",
        "        # 看这里，线性层直接输出结果，不转换为概率矩阵，只修改这里，其余不动。\n",
        "        logits = self.linear(x)\n",
        "        # print(logits.shape)\n",
        "\n",
        "        if targets is not None:\n",
        "\n",
        "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
        "            return logits, loss\n",
        "        else:\n",
        "            return logits\n",
        "        print(\"Model parameters:\", sum([m.numel() for m in self.parameters()]))"
      ],
      "metadata": {
        "id": "omjpq1YMSgQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 再来一次实例化各种功能，再启动一次训练\n",
        "model = SimpleNotStupidModel(MASTER_CONFIG)\n",
        "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
        "logits, loss = model(xs, ys)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "train(model, optimizer)\n",
        "\n",
        "# loss开窍了，下降了很多"
      ],
      "metadata": {
        "id": "XzvPaKmrTnJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 推理函数（输出结果就别纠结其效果了，权重都没保存，就是根据模型初始化生成的随机数组成的矩阵做的推理）\n",
        "def generate(model, config=MASTER_CONFIG, max_new_tokens=20):\n",
        "    # 生成随机数，作为输入数据,5行一列，代表输入5个字符。 这个地方可以自行替换其他随机数测试。\n",
        "    idx = torch.zeros(5, 1).long()\n",
        "    print(idx[:, -config['context_window']:])\n",
        "    for _ in range(max_new_tokens):\n",
        "        # 因为推理的时候，依赖后面的n个token，所以滑动窗口要从后往前选择输入数据的倒数几个token，这个是超过字符数量会对输入进行截断，只选取最后几个token：idx[:, -config['context_window']:]\n",
        "        logits = model(idx[:, -config['context_window']:])\n",
        "        # print(logits.size())\n",
        "        # 得到模型输出的结果，进行解码，这里logits[:, -1, :]挺抽象的，实际上第一维度是输入的字符数，第二维度是时间步，第三维度是词表\n",
        "        # 即，对每一步的解码结果，取最后一个时间步的数据，作为输出的数据。解码的过程是第一次解码，输入5个token，第二次解码依赖的是原来5个token的最后4个，加上上一步解码生成的一个，也是5个token，如此循环。\n",
        "        last_time_step_logits = logits[:, -1, :]\n",
        "        # print('last_time_step_logits')\n",
        "        # print(last_time_step_logits.shape)\n",
        "        # 计算概率分布\n",
        "        p = F.softmax(last_time_step_logits, dim=-1)\n",
        "        # print('p_shape')\n",
        "        # print(p.shape)\n",
        "        # 根据概率分布计算下一个token，这里使用 torch.multinomial做的是随机采样\n",
        "        idx_next = torch.multinomial(p, num_samples=1)\n",
        "        # print('idx_next_shape')\n",
        "        # print(idx_next.shape)\n",
        "        # 将新的idx通过张量拼接写入到解码序列中\n",
        "        idx = torch.cat([idx, idx_next], dim=-1)\n",
        "    # 使用之前定义的解码函数，将ID转换为汉字，我们得到的5行21列的数据，来源于每一个输入字符作为开始位置，生成20个字符。 因为5个输入都是0，在词表中编号为0的数据是'\\n'。\n",
        "    print(idx.shape)\n",
        "    return [decode(x) for x in idx.tolist()]\n",
        "\n",
        "generate(model)\n"
      ],
      "metadata": {
        "id": "kUodzY0sTnv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 将LlaMa优化部分加入到上面的notStupidModel\n",
        "\n",
        "需要做的有三部分:\n",
        "\n",
        "  1.RMS_Norm\n",
        "\n",
        "  2.ROPE\n",
        "\n",
        "  3.SwiGLU"
      ],
      "metadata": {
        "id": "eJr0ezIHZC8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMSNorm\n",
        "### RMSNorm快速了解\n",
        "\n",
        "norm，做标准化，训练过程中的张量标准化操作，通过计算均值和方差，将样本进行归一化。\n",
        "在大学课程《概率与统计》我们学过，样本的均值代表样本的特征，而方差代表离散程度。\n",
        "\n",
        "因此，通过计算，让数据变为均值为0，方差为1的数据。 这样可以使数据服从标准的正态分布。\n",
        "\n",
        "记得大学时候，老师讲这一段的时候，着重强调：“高斯分布，正态分布”，也可以叫自然分布，自然界的很多统计情况，几乎都满足高斯分布。 两边向中心靠拢，超过中心的，随着逐渐增大，会越来越少，没超过中心的，距离中心越远，数量也越来越少。而分布的众数永远都是在中间。 数学之美。\n",
        "\n",
        "使用均值和方差计算数据的标准差，这样既保留了数据的异常值，同时维持数据的异常结构，这样可以稳定梯度，让梯度变化更稳定，减少梯度消失或者爆炸的问题，因为维持了异常结构，也能减少过拟合问题，增强泛化能力。\n",
        "\n",
        "RMSNorm出来之前，广泛使用的batch_normlize，针对批次数据做标准化。标准化的数值是一个batch作为一个样本总体，计算其均值与方差。\n",
        "\n",
        "而后，又出现了layer_norm，其是针对每个token的特征向量做归一化处理（不知道特征向量，请看本人之前的rope文章。应该可以理解token和特征向量的关系。）依旧需要计算均值和方差。\n",
        "\n",
        "\n",
        "RMSNorm和layer_norm的主要区别在于RMSNorm不需要同时计算均值和方差两个统计量，而只需要计算均方根这一个统计量。在模型表现效果几乎与layer_norm持平的前提下，节省7%-64%的计算量。\n",
        "\n",
        "猜想：  既然都平方根了，突然想起程序员之神，约翰卡马克的快速平方根倒数算法了。让所有人直呼waht the f**k的神来一笔。  当然，也有可能这么经典的数值计算方法已经被集成进了numpy。\n",
        "\n",
        "RMS基本介绍差不多了，下面开始实现RMSNorm模块"
      ],
      "metadata": {
        "id": "ypjtkNHnZeS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, layer_shape, eps=1e-8, bias=False):\n",
        "        super(RMSNorm, self).__init__()\n",
        "\n",
        "        # torch中register_parameter()功能为：向我们建立的网络module添加parameter\n",
        "        # 因此，我们需要对pytorch官方封装好的RMSNorm功能模块添加一个可以训练参数的层，命名为scale，并初始化为形状为layer_shape，所有值为1的张量矩阵。\n",
        "        self.register_parameter(\"scale\", nn.Parameter(torch.ones(layer_shape)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 计算Frobenius范数（球某个矩阵中所有元素的平方和再开方得到，该范数用来衡量矩阵的大小，详情请百度）, RMS = 1/sqrt(N) * Frobenius\n",
        "        # 具体来说，torch.linalg.norm(x, dim=(1, 2))计算了x在第1和第2维度上的范数。然后，将结果乘以x[0].numel() ** -.5。x[0].numel()表示x第一个元素（即x的第一行）的元素个数，** -.5表示求平方根的倒数。\n",
        "        ff_rms = torch.linalg.norm(x, dim=(1,2)) * x[0].numel() ** -.5\n",
        "        # print(ff_rms.shape)\n",
        "        # 将ff_rms算子应用于输入的张量x，依据公式，做除法，因为输入向量x是三维的，因此需要对ff_rms进行升两维，也变成三维的张量。这样可以进行元素之间的计算。\n",
        "        raw = x / ff_rms.unsqueeze(-1).unsqueeze(-1)\n",
        "        # print(raw.shape)\n",
        "        # 返回scale缩放后归一化的张量\n",
        "        # print(self.scale[:x.shape[1], :].unsqueeze(0) * raw)\n",
        "        return self.scale[:x.shape[1], :].unsqueeze(0) * raw"
      ],
      "metadata": {
        "id": "-CBTfcF0UCEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNotStupidModel_RMS(nn.Module):\n",
        "    def __init__(self, config=MASTER_CONFIG):\n",
        "      super().__init__()\n",
        "      self.config = config\n",
        "      self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "      # 在这里，我们添加RMS层\n",
        "      self.rms = RMSNorm((config['context_window'], config['d_model']))\n",
        "      self.linear = nn.Sequential(\n",
        "          nn.Linear(config['d_model'], config['d_model']),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(config['d_model'], config['vocab_size']),\n",
        "      )\n",
        "      print(\"Model parameters:\", sum([m.numel() for m in self.parameters()]))\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        x = self.embedding(idx)\n",
        "        # 在这里，添加实例化后的RMS层，承接Embedding层输出的张量\n",
        "        x = self.rms(x)\n",
        "\n",
        "        logits = self.linear(x)\n",
        "        # print(logits.shape)\n",
        "\n",
        "        if targets is not None:\n",
        "\n",
        "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
        "            return logits, loss\n",
        "        else:\n",
        "            return logits\n",
        "        print(\"Model parameters:\", sum([m.numel() for m in self.parameters()]))"
      ],
      "metadata": {
        "id": "dy79Flz4law-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 好啦，这样我们对原来的NotStupidModel添加了RMSNorm，现在执行一下看看\n",
        "model = SimpleNotStupidModel_RMS(MASTER_CONFIG)\n",
        "\n",
        "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
        "\n",
        "logits, loss = model(xs, ys)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train(model, optimizer)\n",
        "\n",
        "# 在同样的训练超参数设置上，加入了RMSNorm的训练速度明显加快。"
      ],
      "metadata": {
        "id": "TDhuuMuHmI0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 增加RoPE\n",
        "\n",
        "具体原理请参考俺的上一篇文章。"
      ],
      "metadata": {
        "id": "442krUUDoa55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rotary_matrix(context_window, embedding_dim):\n",
        "    # 初始化一个0填充，形状为（context_window, embedding_dim, embedding_dim）的张量矩阵，其中context_window为token数量，后面两个embedding_dim组成正方形矩阵，与后面的attention计算对齐格式\n",
        "    R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
        "\n",
        "    # 遍历每一个位置的token\n",
        "    for position in range(context_window):\n",
        "        # 还记得我的上一篇文章中说的，对于特征，两两组合吗，因此需要循环的次数为embedding_dim除以2\n",
        "        for i in range(embedding_dim // 2):\n",
        "            # 设置θ值，采样频率，或者说旋转频率，旋转角都可以，除以embedding_dim防止梯度问题。\n",
        "            theta = 10000. ** (-2. * (i - 1) / embedding_dim)\n",
        "            # 根据欧拉公式，计算旋转的角度，分别有sin 和cos，将计算拉到复数空间，并将旋转角度应用在上面的0填充的矩阵\n",
        "            m_theta = position * theta\n",
        "            R[position, 2 * i, 2 * i] = np.cos(m_theta)\n",
        "            R[position, 2 * i, 2 * i + 1] = -np.sin(m_theta)\n",
        "            R[position, 2 * i + 1, 2 * i] = np.sin(m_theta)\n",
        "            R[position, 2 * i + 1, 2 * i + 1] = np.cos(m_theta)\n",
        "            # 得到的结果是旋转位置编码矩阵，到这里还没覆盖到attention\n",
        "    return R"
      ],
      "metadata": {
        "id": "yLSwbUqLmS1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 接下来创建注意力机制"
      ],
      "metadata": {
        "id": "hYAhw643lTX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 此为单头注意力机制\n",
        "class RoPEMaskedAttentionHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        # 计算Q权重矩阵\n",
        "        self.w_q = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
        "        # 计算K权重矩阵\n",
        "        self.w_k = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
        "        # 计算V权重矩阵\n",
        "        self.w_v = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
        "        # 获得旋转位置编码矩阵，接下来会覆盖Q和K权重矩阵\n",
        "        self.R = get_rotary_matrix(config['context_window'], config['d_model'])\n",
        "\n",
        "\n",
        "    # 这里将上一个代码块中实现的创建旋转位置编码的功能函数原封不动的拿过来\n",
        "    def get_rotary_matrix(context_window, embedding_dim):\n",
        "        # 初始化一个0填充，形状为（context_window, embedding_dim, embedding_dim）的张量矩阵，其中context_window为token数量，后面两个embedding_dim组成正方形矩阵，与后面的attention计算对齐格式\n",
        "        R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
        "\n",
        "        # 遍历每一个位置的token\n",
        "        for position in range(context_window):\n",
        "            # 还记得我的上一篇文章中说的，对于特征，两两组合吗，因此需要循环的次数为embedding_dim除以2\n",
        "            for i in range(embedding_dim // 2):\n",
        "                # 设置θ值，采样频率，或者说旋转频率，旋转角都可以，除以embedding_dim防止梯度问题。\n",
        "                theta = 10000. ** (-2. * (i - 1) / embedding_dim)\n",
        "                # 根据欧拉公式，计算旋转的角度，分别有sin 和cos，将计算拉到复数空间，并将旋转角度应用在上面的0填充的矩阵\n",
        "                m_theta = position * theta\n",
        "                R[position, 2 * i, 2 * i] = np.cos(m_theta)\n",
        "                R[position, 2 * i, 2 * i + 1] = -np.sin(m_theta)\n",
        "                R[position, 2 * i + 1, 2 * i] = np.sin(m_theta)\n",
        "                R[position, 2 * i + 1, 2 * i + 1] = np.cos(m_theta)\n",
        "                # 得到的结果是旋转位置编码矩阵，到这里还没覆盖到attention\n",
        "        return R\n",
        "\n",
        "    def forward(self, x, return_attn_weights=False):\n",
        "        # 前向传播时，输入矩阵的形状为(batch, sequence length, dimension)\n",
        "\n",
        "        b, m, d = x.shape  # batch size, sequence length, dimension\n",
        "\n",
        "        # 线性变换Q,K,V\n",
        "        q = self.w_q(x)\n",
        "        k = self.w_k(x)\n",
        "        v = self.w_v(x)\n",
        "\n",
        "        # 将旋转位置编码应用于Q和K，其中torch.bmm为矩阵做外积，transpose是转置，对Q矩阵转置，并与旋转位置编码做外积，再转置回原状，Q便应用了旋转位置编码。\n",
        "        # 考虑到输入文本的长度，因此对位置编码矩阵在第一维度做截断，因为长了也没用，与文本长度一样。\n",
        "        q_rotated = (torch.bmm(q.transpose(0, 1), self.R[:m])).transpose(0, 1)\n",
        "        # 同理对K也应用旋转位置编码进行覆盖\n",
        "        k_rotated = (torch.bmm(k.transpose(0, 1), self.R[:m])).transpose(0, 1)\n",
        "\n",
        "        # 对注意力机制点积进行等比例缩放，防止attention张量过长引发梯度爆炸，对应\n",
        "        activations = F.scaled_dot_product_attention(\n",
        "            q_rotated, k_rotated, v, dropout_p=0.1, is_causal=True\n",
        "        )\n",
        "        # 如果return_attn_weights参数置为1，则需要对attention进行掩码，因为在学习的时候，希望模型能依据前n个token去预测token，而不是开卷考试。\n",
        "        if return_attn_weights:\n",
        "            # 创建注意力掩码矩阵，其中torch.tril函数为：对于矩阵，取左下三角，剩下的都置0\n",
        "            attn_mask = torch.tril(torch.ones((m, m)), diagonal=0)\n",
        "            # 计算注意力机制的权重矩阵，并对最后一维度做归一化，（突击检查）为什么是最后一维！因为最后一维度是每个token的特征向量！\n",
        "            attn_weights = torch.bmm(q_rotated, k_rotated.transpose(1, 2)) / np.sqrt(d) + attn_mask\n",
        "            attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "            return activations, attn_weights\n",
        "\n",
        "        return activations"
      ],
      "metadata": {
        "id": "IL-iVZ9TlQR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 单头注意力机制实现完毕，下面实现多头注意力机制\n",
        "class RoPEMaskedMultiheadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        # 一个注意力机制头对象构建完毕了，多头的，首先多次创建这个对象。生成多个注意力机制头，塞到一个列表里。\n",
        "        self.heads = nn.ModuleList([\n",
        "            RoPEMaskedAttentionHead(config) for _ in range(config['n_heads'])\n",
        "        ])\n",
        "        # 在模型结构上，创建一个线性层（隐藏层），用于线型输出注意力机制头输出的张量矩阵，寻找多头之间的特征，但是更主要的是，x经过多头计算后形状改变了，创建线性层，让张量矩阵变回原来输入的形状。\n",
        "        # 同时为了防止过拟合，使用随机神经元失活，比率0.1\n",
        "        # 线性层输入形状：注意力机制的头数，乘以矩阵的维度，关联到俺的上一篇文章，就是key矩阵，在多头之间共享权重，减少计算的思维。 输出为：模型的embedding维度数\n",
        "        self.linear = nn.Linear(config['n_heads'] * config['d_model'], config['d_model'])\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 输入矩阵形状x： (batch, sequence length, dimension)\n",
        "\n",
        "        # 每一个注意力机制头，都传入X进行计算。（这个地方开启并行执行会不会快一些，但是不知道pytorch是不是自动调用并行）\n",
        "        heads = [h(x) for h in self.heads]\n",
        "        # 输入张量x经过多个头计算attention（同时，attention是已经覆盖了RoPE的），重新拼接成新的矩阵，重新放入变量x。到这里你应该觉得：那矩阵形状不就变了吗\n",
        "        x = torch.cat(heads, dim=-1)\n",
        "\n",
        "        # 这不，线性层的作用来了\n",
        "        x = self.linear(x)\n",
        "\n",
        "        # 随机失活一下，防止过拟合\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mtsFDKYEpRO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Llama 32个注意力机制头，我们来8个吧\n",
        "\n",
        "MASTER_CONFIG.update({\n",
        "    'n_heads': 8,\n",
        "})"
      ],
      "metadata": {
        "id": "venB2Q6ttGaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 我们已经创建完了所需要的算子，  现在积木已创建完毕，将这些积木组合起来！！！！\n",
        "class RopeModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Embedding层\n",
        "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "\n",
        "        # RMSNorm层\n",
        "        self.rms = RMSNorm((config['context_window'], config['d_model']))\n",
        "\n",
        "        # 旋转位置编码器+注意力机制\n",
        "        self.rope_attention = RoPEMaskedMultiheadAttention(config)\n",
        "\n",
        "        # 线性层+激活函数变为非线性输出！\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(config['d_model'], config['d_model']),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # 最终的输出，因为需要解码，因为输出的维度与词表大小统一！！！\n",
        "        self.last_linear = nn.Linear(config['d_model'], config['vocab_size'])\n",
        "\n",
        "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
        "    # 前向传播\n",
        "    def forward(self, idx, targets=None):\n",
        "        # embedding，不解释\n",
        "        x = self.embedding(idx)\n",
        "        # 归一化数值，不解释\n",
        "        x = self.rms(x)\n",
        "        # 相加，解释一下，因为attention是要覆盖到原矩阵的，想象两个形状一样的矩阵为两张纸，左手一张纸，右手一张纸，双手合十，啪！覆盖。 使用加算，就是将两个矩阵中的元素按位置相加！直接覆盖值！\n",
        "        x = x + self.rope_attention(x)\n",
        "        # 再归一化！\n",
        "        x = self.rms(x)\n",
        "        # 因为直接计算归一化的数值可能出现梯度问题，因此把归一化的值作为修正系数，再覆盖！\n",
        "        x = x + self.linear(x)\n",
        "        # 到这里，才是最终输出vocab数量的神经元输出！！！！！！\n",
        "        logits = self.last_linear(x)\n",
        "\n",
        "        # 训练阶段有目标值\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
        "            return logits, loss\n",
        "        # 验证或者推理阶段，目标值y没有！只有结果，没有loss！\n",
        "        else:\n",
        "            return logits"
      ],
      "metadata": {
        "id": "UeNlI3_1tO4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 再跑一下！\n",
        "model = RopeModel(MASTER_CONFIG)\n",
        "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
        "logits, loss = model(xs, ys)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "train(model, optimizer)\n",
        "# loss下降了百分之0.1！"
      ],
      "metadata": {
        "id": "P_8GnwS9vdO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SwiGLU实现\n",
        "\n",
        "将swish和glu结合起来。这两个激活函数单独拿出来都很强，结合起来。\n",
        "\n",
        "这玩意挺玄学，说它不好吧，但是这玩意确实比relu这败家子保留更多语义特征参数，不至于某个权重突然到了小于0的区间，然后糊里糊涂的消失。说它好吧，它的计算量确实挺大。   \n",
        "\n",
        "swish用了sigmoid，GLU用了门控结构（门控结构思想，可以学习一下RNN,GRU,LSTM什么的）"
      ],
      "metadata": {
        "id": "b5nASknEx_Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SwiGLU(nn.Module):\n",
        "\n",
        "    def __init__(self, size):\n",
        "        super().__init__()\n",
        "        # 定义一个门控的线性层，输入输出都是门控结构的尺寸\n",
        "        self.linear_gate = nn.Linear(size, size)\n",
        "        # 门控结构主干线性层\n",
        "        self.linear = nn.Linear(size, size)\n",
        "        # 初始化一个随机数作为beta系数\n",
        "        self.beta = torch.randn(1, requires_grad=True)\n",
        "\n",
        "        # nn.Parameter用于指定某一层参数为可学习的，即本来不能通过训练更改参数，现在变成了可以经过训练来更新的参数。\n",
        "        self.beta = nn.Parameter(torch.ones(1))\n",
        "        # 将随机数beta指定为一个名为beta的神经网络层\n",
        "        self.register_parameter(\"beta\", self.beta)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Swish门控但愿的计算：（从括号里开始）对于原始输入的数据张量，经过线性变换乘以beta系数，再经过sigmoid变换为0-1之间的值，再乘以原数据经过门控线性变换。总的来说，线型输出经过非线性变换，再应用到线性变换的结果，元素按位置相乘，修正原本数据张量，就是这个门控结构做的事情。\n",
        "        swish_gate = self.linear_gate(x) * torch.sigmoid(self.beta * self.linear_gate(x))\n",
        "        # 将门控结构输出的值再按位乘以线型输出的原数据张量\n",
        "        # 为啥这么做，我不知道，但是论文复现的代码就是这样滴，有兴趣可以研究一下，我没研究过。\n",
        "        out = swish_gate * self.linear(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "NMwrHYifvk9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 再将swiglu添加进上面的模型\n",
        "class RopeModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "        self.rms = RMSNorm((config['context_window'], config['d_model']))\n",
        "        self.rope_attention = RoPEMaskedMultiheadAttention(config)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(config['d_model'], config['d_model']),\n",
        "            # 在这里，增加了SwiGLU层\n",
        "            SwiGLU(config['d_model']),\n",
        "        )\n",
        "        self.last_linear = nn.Linear(config['d_model'], config['vocab_size'])\n",
        "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        x = self.embedding(idx)\n",
        "        x = self.rms(x)\n",
        "        x = x + self.rope_attention(x)\n",
        "        x = self.rms(x)\n",
        "        x = x + self.linear(x)\n",
        "        logits = self.last_linear(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # Calculate cross-entropy loss if targets are provided\n",
        "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
        "            return logits, loss\n",
        "\n",
        "        else:\n",
        "            return logits"
      ],
      "metadata": {
        "id": "5_YNKKgK2j9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 一二三四！再来一次！\n",
        "model = RopeModel(MASTER_CONFIG)\n",
        "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
        "logits, loss = model(xs, ys)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "train(model, optimizer)"
      ],
      "metadata": {
        "id": "MMcwiVjH3Doo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OK！ 现在我们更新一下，隐藏层维度堆叠多少层，我们先来4层尝尝咸淡！！！！\n",
        "MASTER_CONFIG.update({\n",
        "    'n_layers': 4,\n",
        "})"
      ],
      "metadata": {
        "id": "JLds9ex53JmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 现在我们拥有了所有的算子，RMS，ROPE,SWIGLU，我们搭建我们的LlaMa！ 首先实现LlaMa的功能块，然后堆叠。\n",
        "# 功能没什么好讲的，如果仔细看到了这里，下面的每一行代码都难不住你。\n",
        "class LlamaBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.rms = RMSNorm((config['context_window'], config['d_model']))\n",
        "        self.attention = RoPEMaskedMultiheadAttention(config)\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(config['d_model'], config['d_model']),\n",
        "            SwiGLU(config['d_model']),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.rms(x)\n",
        "        x = x + self.attention(x)\n",
        "\n",
        "        x = self.rms(x)\n",
        "        x = x + self.feedforward(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "yMboVV6v37jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 看一下我们的超参数字典\n",
        "MASTER_CONFIG"
      ],
      "metadata": {
        "id": "DFZhJfct5UsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 用config字典，创建llama的功能块\n",
        "block = LlamaBlock(MASTER_CONFIG)\n",
        "\n",
        "# 生成一条随机数据，丢到这个llama功能块里，看一下是不是有bug\n",
        "random_input = torch.randn(MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'], MASTER_CONFIG['d_model'])\n",
        "\n",
        "# 执行以下看看输出\n",
        "output = block(random_input)\n",
        "output.shape"
      ],
      "metadata": {
        "id": "RaLaako55BVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 现在，我们组装LlaMa\n",
        "from collections import OrderedDict\n",
        "class Llama(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        # Embedding不解释\n",
        "        self.embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "        # 根据传入的堆叠层数，创建Llama功能块，注意OrderedDict为一种特殊类型的字典数据，保留字典写入的顺序，先插入的数据在前，后插入的数据在后。\n",
        "        # 这里，我们将llama的功能块堆叠4层\n",
        "        self.llama_blocks = nn.Sequential(\n",
        "            OrderedDict([(f\"llama_{i}\", LlamaBlock(config)) for i in range(config['n_layers'])])\n",
        "        )\n",
        "        # FFN层，包含：线性层、激活函数非线性变换、再用线性层输出最终解码数值。\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(config['d_model'], config['d_model']),\n",
        "            SwiGLU(config['d_model']),\n",
        "            nn.Linear(config['d_model'], config['vocab_size']),\n",
        "        )\n",
        "\n",
        "        # 看看咱们的大模型多少参数！\n",
        "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # embedding嵌入\n",
        "        x = self.embeddings(idx)\n",
        "        # Llama模型计算\n",
        "        x = self.llama_blocks(x)\n",
        "        # FFN计算，得到logits\n",
        "        logits = self.ffn(x)\n",
        "\n",
        "        # 推理阶段没有目标值，只输出结果\n",
        "        if targets is None:\n",
        "            return logits\n",
        "        # 训练阶段，有目标值，需要输出结果，以及loss，用于反向传播更新权重！\n",
        "        else:\n",
        "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
        "            return logits, loss"
      ],
      "metadata": {
        "id": "Uz9J_gDi5tzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 开始训练咱们的Llama\n",
        "llama = Llama(MASTER_CONFIG)\n",
        "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
        "logits, loss = llama(xs, ys)\n",
        "optimizer = torch.optim.Adam(llama.parameters())\n",
        "train(llama, optimizer)"
      ],
      "metadata": {
        "id": "cy-9WvWv6oW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 练它一万轮！有条件的开启，本厮实在是不愿意等了。\n",
        "MASTER_CONFIG.update({\n",
        "    'epochs': 10000,\n",
        "})\n",
        "\n",
        "#train(llama, optimizer, scheduler=None, config=MASTER_CONFIG)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OTnYegOT7wGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 再看一下推理效果（实际上也没什么效果-。-）\n",
        "# 别忘了generate里面的输入数据是咱们弄的5个0，如果替换为encode之后的数也是可以的！组成列表，转换tensor，这个应该没问题的吧~\n",
        "generated_text = generate(llama, MASTER_CONFIG, 500)[0]\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "FidF8_B88el9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 下面是测试集跑一下\n",
        "# 获取测试集的特征值和目标值\n",
        "xs, ys = get_batches(dataset, 'test', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
        "\n",
        "# 丢进Llama获取loss\n",
        "logits, loss = llama(xs, ys)\n",
        "\n",
        "print(loss)\n",
        "# 4.7326！"
      ],
      "metadata": {
        "id": "U-kMQIhX8_1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 还有优化的点哦，别忘了optimizer！以及学习率调度器！\n",
        "# 调整参数再来一次！\n",
        "\n",
        "MASTER_CONFIG.update({\n",
        "    \"epochs\": 1000\n",
        "})\n",
        "\n",
        "# 学习率优化器选择余弦退火\n",
        "llama_with_cosine = Llama(MASTER_CONFIG)\n",
        "\n",
        "llama_optimizer = torch.optim.Adam(\n",
        "    llama.parameters(),\n",
        "    betas=(.9, .95),\n",
        "    weight_decay=.1,\n",
        "    eps=1e-9,\n",
        "    lr=1e-3\n",
        ")\n",
        "# 余弦退火学习率优化器，让学习率逐渐减小，在结束时达到最低值。 详细可以百度，这种文章很多。\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(llama_optimizer, 300, eta_min=1e-5)\n",
        "\n",
        "# 跑一下！\n",
        "train(llama_with_cosine, llama_optimizer, scheduler=scheduler)"
      ],
      "metadata": {
        "id": "BAENCwZ69PC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存你的模型！\n",
        "\n",
        "torch.save(llama, 'llama_model.pth')\n",
        "\n",
        "\n",
        "torch.save(llama.state_dict(), 'llama_model_params.pth')"
      ],
      "metadata": {
        "id": "dTixgsvf-FFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 当然了，也可以换一种方式保存，并加载推理"
      ],
      "metadata": {
        "id": "8zh_tskHRxul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存模型权重\n",
        "model_save_path = \"./hf_model_save/pytorch_model.bin\"\n",
        "torch.save(llama_with_cosine.state_dict(), model_save_path)"
      ],
      "metadata": {
        "id": "dHmAVWIDN0_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 生成一个config文件\n",
        "import json\n",
        "\n",
        "config_save_path = \"./hf_model_save/config.json\"\n",
        "with open(config_save_path, 'w') as f:\n",
        "    json.dump(MASTER_CONFIG, f)"
      ],
      "metadata": {
        "id": "-gzvDQXsQOGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存optimizer和学习率调度器的状态，方便继续微调\n",
        "optimizer_save_path = \"./hf_model_save/optimizer.pt\"\n",
        "torch.save(llama_optimizer.state_dict(), optimizer_save_path)\n",
        "\n",
        "scheduler_save_path = \"./hf_model_save/scheduler.pt\"\n",
        "torch.save(scheduler.state_dict(), scheduler_save_path)\n"
      ],
      "metadata": {
        "id": "1R9Z-brxQOnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 接下来是加载模型\n",
        "llama_with_cosine = Llama(MASTER_CONFIG)\n",
        "\n",
        "# 加载模型权重\n",
        "model_save_path = \"./hf_model_save/pytorch_model.bin\"\n",
        "llama_with_cosine.load_state_dict(torch.load(model_save_path))\n",
        "\n",
        "# 设置为评估模式\n",
        "llama_with_cosine.eval()"
      ],
      "metadata": {
        "id": "7AiW_fOOQSKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载优化器和学习率调度器\n",
        "llama_optimizer.load_state_dict(torch.load(optimizer_save_path))\n",
        "scheduler.load_state_dict(torch.load(scheduler_save_path))\n"
      ],
      "metadata": {
        "id": "kTdrJEcwQXRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 进行推理\n",
        "output = generate(llama_with_cosine, MASTER_CONFIG)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "LwGxwhcTQkju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 接下来可以整点花活儿，比如：部署一个异步的远程服务"
      ],
      "metadata": {
        "id": "LbgaH8amTfwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8aD8UqE5TzTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 初始化 FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# 模型加载\n",
        "model_path = \"./hf_model_save/pytorch_model.bin\"\n",
        "model = Llama(MASTER_CONFIG)\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "\n",
        "class InputData(BaseModel):\n",
        "    idx: list\n",
        "\n",
        "@app.post(\"/generate/\")\n",
        "async def generate(model, config=MASTER_CONFIG, max_new_tokens=20):\n",
        "    # 生成随机数，作为输入数据,5行一列，代表输入5个字符。 这个地方可以自行替换其他随机数测试。\n",
        "    idx = torch.zeros(5, 1).long()\n",
        "    print(idx[:, -config['context_window']:])\n",
        "    for _ in range(max_new_tokens):\n",
        "        # 因为推理的时候，依赖后面的n个token，所以滑动窗口要从后往前选择输入数据的倒数几个token，这个是超过字符数量会对输入进行截断，只选取最后几个token：idx[:, -config['context_window']:]\n",
        "        logits = model(idx[:, -config['context_window']:])\n",
        "        # print(logits.size())\n",
        "        # 得到模型输出的结果，进行解码，这里logits[:, -1, :]挺抽象的，实际上第一维度是输入的字符数，第二维度是时间步，第三维度是词表\n",
        "        # 即，对每一步的解码结果，取最后一个时间步的数据，作为输出的数据。解码的过程是第一次解码，输入5个token，第二次解码依赖的是原来5个token的最后4个，加上上一步解码生成的一个，也是5个token，如此循环。\n",
        "        last_time_step_logits = logits[:, -1, :]\n",
        "        # print('last_time_step_logits')\n",
        "        # print(last_time_step_logits.shape)\n",
        "        # 计算概率分布\n",
        "        p = F.softmax(last_time_step_logits, dim=-1)\n",
        "        # print('p_shape')\n",
        "        # print(p.shape)\n",
        "        # 根据概率分布计算下一个token，这里使用 torch.multinomial做的是随机采样\n",
        "        idx_next = torch.multinomial(p, num_samples=1)\n",
        "        # print('idx_next_shape')\n",
        "        # print(idx_next.shape)\n",
        "        # 将新的idx通过张量拼接写入到解码序列中\n",
        "        idx = torch.cat([idx, idx_next], dim=-1)\n",
        "    # 使用之前定义的解码函数，将ID转换为汉字，我们得到的5行21列的数据，来源于每一个输入字符作为开始位置，生成20个字符。 因为5个输入都是0，在词表中编号为0的数据是'\\n'。\n",
        "    print(idx.shape)\n",
        "    return [decode(x) for x in idx.tolist()]\n"
      ],
      "metadata": {
        "id": "uyxPz5ssVjIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 在colab里启动还是挺麻烦的。  建议把所有代码整理一下，在服务器，或者个人电脑里运行\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# 启动 FastAPI 应用\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "id": "V1450N7WQmH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 服务部署成功后，可以发送请求测试效果\n",
        "import requests\n",
        "\n",
        "input_data = {\"idx\": [[0]]}  # 根据需求提供输入数据\n",
        "response = requests.post(\"http://localhost:8000/generate/\", json=input_data)\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "id": "sDIKoI1kUCXo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}